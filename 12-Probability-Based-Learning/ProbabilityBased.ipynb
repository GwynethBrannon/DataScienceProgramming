{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability-based Learning\n",
    "Probabilty-based prediction approaches are heavily based on **Bayes' Theorem**, and this session introduces the fundamentals of **probability theory**.\n",
    "\n",
    "\\begin{equation}\n",
    "P(t|\\mathbf{d})=\\frac{P(\\mathbf{d}|t) \\times P(t)}{P(\\mathbf{d})}\n",
    "\\end{equation}\n",
    "\n",
    "- A Naive Bayes' classifier naively assumes that each of the descriptive features in a domain is conditionally independent of all of the other descriptive features, given the state of the target feature. \n",
    "- This assumption, although often wrong, enables the Naive Bayes' model to maximally factorise the representation that it uses of the domain. \n",
    "- Surprisingly, given the naivety and strength of the assumption it depends upon, a Naive Bayes' model often performs reasonably well. \n",
    "\n",
    "<p style=\"font-size: 1.5em; font-weight: bold;\">Reading</p>\n",
    "\n",
    "- Chapter 5 of [Fundamentals of Machine Learning for Predictive Data Analytics](https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics)\n",
    "- [Chapter 6 Slides 'A'](http://131.96.197.204/~pmolnar/mlbook/BookSlides_6A_Probability-based_Learning.pdf) (internal)\n",
    "- [Chapter 6 Slides 'B'](http://131.96.197.204/~pmolnar/mlbook/BookSlides_6B_Probability-based_Learning.pdf) (internal)\n",
    "\n",
    "Slides are posted on the internal server http://131.96.197.204/~pmolnar/mlbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "**Big Idea**\n",
    "* We can use estimates of likelihoods to determine the most likely prediction that should be made.\n",
    "* More importantly, we revise these predictions based on data we collect and whenever extra evidence becomes available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple dataset for **Meningitis** diagnosis with descriptive features that describe the presence or absence of three common symptoms of the disease: **Headache**, **Fever**, and **Vomiting**.\n",
    "\n",
    "| **ID** | **Headache** | **Fever** | **Vomiting** | **Meningitis**|\n",
    "|--------|--------------|-----------|--------------|---------------|\n",
    "|1 | true | true | false | false|\n",
    "|2 | false | true | false | false|\n",
    "|3 | true | false | true | false|\n",
    "|4 | true | false | true | false|\n",
    "|5 | false | true | false | true|\n",
    "|6 | true | false | true | false|\n",
    "|7 | true | false | true | false|\n",
    "|8 | true | false | true | true|\n",
    "|9 | false | true | false | false|\n",
    "|10 | true | false | true | true|\n",
    "\n",
    "\n",
    "- A **probability function**, $P()$, returns the probability of a feature taking a specific value.\n",
    "- A **joint probability** refers to the probability of an assignment of specific values to multiple different features.\n",
    "- A **conditional probability**  refers to the probability of one feature taking a specific value given that we already know the value of a different feature\n",
    "- A **probability distribution** is a data structure that describes the probability of each possible value a feature can take. The sum of a probability distribution must equal $1.0$.\n",
    "- A **joint probability distribution** is a probability distribution over more than one feature assignment and is written as a multi-dimensional matrix in which each cell lists the probability of a particular combination of feature values being assigned. \n",
    "- The sum of all the cells in a joint probability distribution must be $1.0$. \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{P}(H,F,V,M) = \\left[ \\begin{array}{ll} \n",
    "P(h, f, v, m),| P(\\lnot h, f, v, m)\\\\\n",
    "P(h, f, v, \\lnot m),| P(\\lnot h, f, v, \\lnot m)\\\\\n",
    "P(h, f, \\lnot v, m),| P(\\lnot h, f, \\lnot v, m)\\\\\n",
    "P(h, f, \\lnot v, \\lnot m),| P(\\lnot h, f, \\lnot v, \\lnot m)\\\\\n",
    "P(h, \\lnot f, v, m),| P(\\lnot h, \\lnot f, v, m)\\\\\n",
    "P(h, \\lnot f, v, \\lnot m),| P(\\lnot h, \\lnot f, v, \\lnot m)\\\\\n",
    "P(h, \\lnot f, \\lnot v, m),| P(\\lnot h, \\lnot f, \\lnot v, m) \\\\\n",
    "P(h, \\lnot f, \\lnot v, \\lnot m),| P(\\lnot h, \\lnot f, \\lnot v, \\lnot m) \\\\ \\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a joint probability distribution, we can compute the probability of any event in the domain that it covers by summing over the cells in the distribution where that event is true. \n",
    "- Calculating probabilities in this way is known as \\keyword{summing out}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "$$\n",
    "P(X|Y)=\\frac{P(Y|X)P(X)}{P(Y)}\n",
    "$$\n",
    "\n",
    "\n",
    "After a yearly checkup, a doctor informs their patient that he has both bad news and good news. The bad news is that the patient has tested positive for a serious disease and that the test that the doctor has used is $99\\%$ accurate (i.e., the probability of testing positive when a patient has the disease is 0.99, as is the probability of testing negative when a patient does not have the disease). The good news, however, is that the disease is extremely rare, striking only 1 in 10,000 people. \n",
    "\n",
    "- What is the actual probability that the patient has the disease? \n",
    "- Why is the rarity of the disease good news given that the patient has tested positive for it?\n",
    "\n",
    "$$\n",
    "P(d|t)=\\frac{P(t|d)P(d)}{{P(t)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "P(t) & = & P(t|d)P(d)+P(t|\\lnot d)P(\\lnot d) \\\\\n",
    "& = & (0.99 \\times 0.0001)+(0.01 \\times 0.9999) = 0.0101\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "P(d|t) & = & \\displaystyle\\frac{0.99 \\times 0.0001}{0.0101} \\\\\n",
    "& = & 0.0098\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "P(d|t) & =  & \\frac{0.99 \\times 0.0001}{0.0101} \\\\\n",
    "& = &  0.0098\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Bayes theorem\n",
    "\n",
    "\\begin{alignat*}{4}\n",
    "& P(Y|X)P(X) & = & \\;\\;\\;(X|Y)P(Y) \\\\\n",
    "\\Rightarrow\\;\\;\\; & \\frac{P(X|Y)\\mathbf{P(Y)}}{\\mathbf{P(Y)}} & = & \\;\\;\\; \\frac{P(Y|X)P(X)}{P(Y)}\\\\\n",
    "\\Rightarrow\\;\\;\\; & P(X|Y) & = &\\;\\;\\; \\frac{P(Y|X)P(X)}{P(Y)}\n",
    "\\end{alignat*}\n",
    "\n",
    "- The divisor is the prior probability of the evidence\n",
    "- This division functions as a normalization constant.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "0 \\leq P(X|Y) & \\leq &  1\\\\\n",
    "\\sum_{i} P(X_i|Y) &  =  &  1.0\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- We can calculate this divisor directly from the dataset.\n",
    "\n",
    "$$\n",
    "P(Y)=\\frac{|\\{ \\text{rows where Y is the case} \\}|}{|\\{ \\text{rows in the dataset} \\}|}\n",
    "$$\n",
    "\n",
    "- Or, we can use the **Theorem of Total Probability** to calculate this divisor.\n",
    "\n",
    "$$\n",
    "P(Y)=\\sum_i P(Y|X_i)P(X_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Prediction\n",
    "\n",
    "### Generalized Bayes' Theorem\n",
    "$$\n",
    "P(t=l|\\mathbf{q}[1],\\dots,\\mathbf{q}[m])=\\frac{P(\\mathbf{q}[1],\\dots,\\mathbf{q}[m]| t=l)P(t=l)}{P(\\mathbf{q}[1],\\dots,\\mathbf{q}[m])}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "$$\n",
    "P(\\mathbf{q}[1],\\dots, \\mathbf{q}[m]) =  P(\\mathbf{q}[1]) \\times P(\\mathbf{q}[2] | \\mathbf{q}[1]) \\times \\dots \\times P(\\mathbf{q}[m] |\\mathbf{q}[m-1], \\dots, \\mathbf{q}[2],\\mathbf{q}[1])\n",
    "$$\n",
    "\n",
    "\n",
    "- To apply the chain rule to a conditional probability we just add the conditioning term to each term in the expression: \n",
    "\n",
    "$$\n",
    "P(\\mathbf{q}[1],\\dots,\\mathbf{q}[m] |{t=l}) =  P(\\mathbf{q}[1]|{t=l}) \\times P(\\mathbf{q}[2] | \\mathbf{q}[1], {t=l}) \\times \\dots \\times P(\\mathbf{q}[m] |\\mathbf{q}[m-1], \\dots, \\mathbf{q}[3], \\mathbf{q}[2],\\mathbf{q}[1], {t=l})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Headache** | **Fever** | **Vomiting** | **Meningitis**|\n",
    "|-------------|-----------|--------------|---------------|\n",
    "|1 | true | true | false | false |\n",
    "|2 | false | true | false | false |\n",
    "|3 | true | false | true | false |\n",
    "|4 | true | false | true | false |\n",
    "|5 | false | true | false | true |\n",
    "|6 | true | false | true | false |\n",
    "|7 | true | false | true | false |\n",
    "|8 | true | false | true | true |\n",
    "|9 | false | true | false | false |\n",
    "|10 | true | false | true | true |\n",
    "\n",
    "|**Headache** | **Fever** | **Vomiting** | **Meningitis**|\n",
    "|-------------|-----------|--------------|---------------|\n",
    "| true | false | true | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(M| h, \\lnot f, v)=?\n",
    "$$\n",
    "\n",
    "- In the terms of Bayes' Theorem this problem can be stated as:\n",
    "\n",
    "$$\n",
    "P(M| h, \\lnot f, v)=\\frac{P(h,\\lnot f, v|M) \\times  P(M)}{P(h, \\lnot f, v)}\n",
    "$$\n",
    "- There are two values in the domain of the **Meningitis** feature, **true** and **false**, so we have to do this calculation twice. \n",
    "- We will do the calculation for $m$ first\n",
    "- To carry out this calculation we need to know the following probabilities: ${P(m)}$, ${P(h, \\lnot f, v)}$ and ${P(h, \\lnot f, v\\mid m)}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- We can calculate the required probabilities directly from the data. For example, we can calculate ${P(m)}$ and ${P(h, \\lnot f, v)}$ as follows:\n",
    "\n",
    "\\begin{alignat*}{2}\n",
    "P(m)&=\\frac{\\left| \\{\\mathbf{d}_5, \\mathbf{d}_8, \\mathbf{d}_{10} \\} \\right|}{\\left| \\{\\mathbf{d}_1, \\mathbf{d}_2, \\mathbf{d}_3, \\mathbf{d}_4, \\mathbf{d}_5, \\mathbf{d}_6, \\mathbf{d}_7, \\mathbf{d}_8, \\mathbf{d}_9, \\mathbf{d}_{10} \\} \\right|}=\\frac{3}{10}=0.3\\\\\n",
    "P(h, \\lnot f, v)&=\\frac{\\left| \\{ \\mathbf{d}_3, \\mathbf{d}_4, \\mathbf{d}_6, \\mathbf{d}_7, \\mathbf{d}_8, \\mathbf{d}_{10} \\} \\right|}{\\left| \\{ \\mathbf{d}_1, \\mathbf{d}_2, \\mathbf{d}_3, \\mathbf{d}_4, \\mathbf{d}_5, \\mathbf{d}_6, \\mathbf{d}_7, \\mathbf{d}_8, \\mathbf{d}_9, \\mathbf{d}_{10}\\} \\right|}=\\frac{6}{10}=0.6\\\\\n",
    "\\end{alignat*}\n",
    "\n",
    "\n",
    "- However, as an exercise we will use the chain rule calculate:\n",
    "\n",
    "\\begin{alignat*}{2}\n",
    "P(h, \\lnot f, v\\mid m)&={?}\\\\ \n",
    "\\end{alignat*}\n",
    "\n",
    "- Using the chain rule calculate:\n",
    "\n",
    "\\begin{alignat*}{2}\n",
    "P(h, \\lnot f, v\\mid m)&=P(h\\mid m)  \\times P(\\lnot f \\mid h,m) \\times P(v \\mid \\lnot f, h,m)\\\\\n",
    "&=\\frac{\\left| \\{ \\mathbf{d}_8, \\mathbf{d}_{10} \\} \\right|}{\\left| \\{ \\mathbf{d}_5, \\mathbf{d}_8, \\mathbf{d}_{10} \\} \\right|} \\times  \\frac{\\left| \\{ \\mathbf{d}_8, \\mathbf{d}_{10} \\} \\right|}{\\left| \\{ \\mathbf{d}_8, \\mathbf{d}_{10} \\} \\right|} \\times \\frac{\\left| \\{ \\mathbf{d}_8, \\mathbf{d}_{10} \\} \\right|}{\\left| \\{ \\mathbf{d}_8, \\mathbf{d}_{10} \\} \\right|}\\\\ \n",
    "&=\\frac{2}{3} \\times  \\frac{2}{2} \\times \\frac{2}{2} = 0.6666\\\\ \n",
    "\\end{alignat*}\n",
    "\n",
    "- So the calculation of $P(m|h, \\lnot f, v)$ is: \n",
    "\n",
    "\\begin{alignat*}{2}\n",
    "P(m| h, \\lnot f, v)&=\\frac{ \\left(\n",
    "\\begin{aligned}\n",
    "P(&h|m)  \\times P(\\lnot f|h,m)\\\\\n",
    "&\\times P(v|\\lnot f, h,m) \\times P(m)\n",
    "\\end{aligned}\n",
    "\\right)\n",
    "}{P(h, \\lnot f, v)}\\\\\n",
    "&=\\frac{0.6666 \\times 0.3}{0.6}=0.3333\n",
    "\\end{alignat*}\n",
    "\n",
    "- The corresponding calculation for $P(\\lnot m|h, \\lnot f, v)$ is: \n",
    "\n",
    "\\begin{alignat*}{2}\n",
    "P(\\lnot m \\mid h, \\lnot f, v)&=\\frac{\n",
    "P(h, \\lnot f, v\\mid \\lnot m) \\times P(\\lnot m)\n",
    "}{P(h, \\lnot f, v)}\\\\\n",
    "&=\\frac{ \\left(\n",
    "\\begin{aligned}\n",
    "P(&h|\\lnot m)  \\times P(\\lnot f\\mid h,\\lnot m)\\\\\n",
    "&\\times P(v|\\lnot f, h,\\lnot m) \\times P(\\lnot m)\n",
    "\\end{aligned} \n",
    "\\right)\n",
    "}{P(h, \\lnot f, v)}\\\\\n",
    "&=\\frac{0.7143 \\times 0.8 \\times 1.0 \\times 0.7}{0.6}=0.6667\n",
    "\\end{alignat*}\n",
    "\n",
    "\n",
    "$$\n",
    "P(m| h, \\lnot f, v)=0.3333\n",
    "$$\n",
    "$$\n",
    "P(\\lnot m| h, \\lnot f, v)=0.6667\n",
    "$$\n",
    "- These calculations tell us that it is twice as probable that the patient does not have meningitis than it is that they do even though the patient is suffering from a headache and is vomiting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; padding: 2em;\">\n",
    "<p style=\"font-size: 2em; margin-bottom: 2em\">The Paradox of the False Positive</p>\n",
    "\n",
    "- The mistake of forgetting to factor in the prior gives rise to the **paradox of the false positive** which states that in order to make predictions about a rare event the model has to be as accurate as the prior of the event is rare or there is a significant chance of **false positives** predictions (i.e., predicting the event when it is not the case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; padding: 2em;\">\n",
    "<p style=\"font-size: 2em; margin-bottom: 2em\">Bayesian MAP Prediction Model </p>\n",
    "\n",
    "\n",
    "\\begin{alignedat}{2}\n",
    "\\mathbb{M}_{MAP}(\\mathbf{q})&= argmax_{l \\in levels(t)} P(t=l \\mid \\mathbf{q}[1], \\dots, \\mathbf{q}[m])\\\\\n",
    "&= argmax_{l \\in levels(t)} \\frac{P(\\mathbf{q}[1], \\dots, \\mathbf{q}[m] \\mid t=l) \\times P(t=l)}{  P(\\mathbf{q}[1], \\dots, \\mathbf{q}[m])}\n",
    "\\end{alignedat}\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; padding: 2em;\">\n",
    "<p style=\"font-size: 2em; margin-bottom: 2em\">Bayesian MAP Prediction Model (without normalization)</p>\n",
    "$\n",
    "\\mathbb{M}_{MAP}(\\mathbf{q})= argmax_{l \\in levels(t)}  P(\\mathbf{q}[1], \\dots, \\mathbf{q}[m] \\mid t=l) \\times P(t=l)\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; padding: 2em;\">\n",
    "<p style=\"font-size: 2em; margin-bottom: 2em\">Curse of Dimensionality</p>\n",
    "\n",
    "As the number of descriptive features grows the number of potential conditioning events grows. Consequently, an exponential increase is required in the size of the dataset as each new descriptive feature is added to ensure that for any conditional probability there are enough instances in the training dataset matching the conditions so that the resulting probability is reasonable. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Approach: The Naive Bayes' Classifier\n",
    "$$\n",
    "\\mathbb{M}(\\mathbf{q})= argmax_{l \\in levels(t)} \\left( \\prod_{i=1}^m P(\\mathbf{q}[i]\\mid t=l) \\right) \\times P(t=l)\n",
    "$$\n",
    "\n",
    "Naive Bayes' is simple to train!\n",
    "1. calculate the priors for each of the target levels\n",
    "2. calculate the conditional probabilities for each feature given each target level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "992px",
    "left": "0px",
    "right": "1068px",
    "top": "106px",
    "width": "264px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
